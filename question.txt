ShopSmart is an online retail platform that places a high value on customer feedback. Each month, the company receives hundreds of comments from shoppers regarding product quality, delivery speed, customer service, and more. To automatically understand and cluster this feedback, ShopSmart's data science team uses text embeddings to capture the semantic meaning behind each comment.

    As part of a pilot project, ShopSmart has curated a collection of 25 feedback phrases that represent a variety of customer sentiments. Examples of these phrases include comments like “Fast shipping and great service,” “Product quality could be improved,” “Excellent packaging,” and so on. Due to limited processing capacity during initial testing, you have been tasked with determine which pair(s) of 5 of these phrases are most similar to each other. This similarity analysis will help in grouping similar feedback to enhance the company’s understanding of recurring customer issues.

    ShopSmart has written a Python program that has the 5 phrases and their embeddings as an array of floats. It looks like this:

    embeddings = \{"The quality exceeds the price.":[-0.050457071512937546,-0.034066375344991684,-0.10696785151958466,-0.03518003225326538,0.11867549270391464,-0.08566565811634064,-0.017789902165532112,0.3559122681617737,-0.04817265644669533,-0.14437519013881683,0.14620272815227509,0.015005768276751041,-0.34517550468444824,0.022687122225761414,0.2908063530921936,0.17681391537189484,-0.20993797481060028,-0.286237508058548,-0.022829897701740265,0.04428914561867714,0.08435212075710297,0.04840109869837761,-0.03081108257174492,-0.04203328490257263,-0.0324387289583683,-0.23552344739437103,0.0033713006414473057,0.02891216054558754,0.0676758736371994,-0.11616263538599014,0.05408358573913574,-0.18183964490890503,0.23757942020893097,-0.34266263246536255,-0.19920121133327484,-0.021787632256746292,0.0973161906003952,0.032724279910326004,0.07030294835567474,-0.1132500022649765,0.09360401332378387,0.028341054916381836,-0.09657375514507294,0.1291838139295578,0.12198790162801743,0.019260495901107788,0.02211601845920086,0.058595310896635056,-0.07481467723846436,0.012935514561831951],"I had difficulty tracking my order.":[0.12442748248577118,0.0033108799252659082,-0.04050052911043167,-0.20462776720523834,-0.08943995088338852,-0.023299355059862137,-0.023514946922659874,0.0016102041117846966,-0.04690669849514961,0.11284710466861725,0.07447169721126556,-0.06517043709754944,-0.056577544659376144,-0.016893187537789345,-0.05214250832796097,0.07330133765935898,0.07083743065595627,0.036773864179849625,-0.3843700587749481,0.2656095623970032,0.07034464925527573,0.02235998772084713,0.3134094178676605,-0.2508260905742645,-0.15732069313526154,-0.05469881370663643,0.05953424051403999,-0.25119566917419434,-0.005624645855277777,0.09147267788648605,0.23099161684513092,-0.11666616797447205,0.1610165536403656,0.01162657793611288,-0.10237548500299454,0.05660834535956383,-0.006267572287470102,0.09523013979196548,0.10681052505970001,0.2604353427886963,0.01636960543692112,0.27423325181007385,0.101882703602314,0.02844276838004589,0.07243897020816803,0.18134382367134094,-0.2525508403778076,-0.03859099745750427,-0.08500491827726364,0.0959693193435669],"The website is user-friendly.":[-0.17558817565441132,-0.15948393940925598,-0.4088399410247803,0.09409292787313461,0.1044178232550621,-0.19364051520824432,-0.15688647329807281,0.22987505793571472,0.04376717284321785,0.028831787407398224,0.07759906351566315,-0.09389811754226685,-0.13740554451942444,-0.03180262818932533,0.22506976127624512,-0.02987077087163925,-0.2480572611093521,-0.08526156842708588,-0.08441739529371262,0.06123507767915726,0.2639017701148987,0.08117057383060455,0.024302469566464424,-0.1449381709098816,0.08207967877388,-0.005746876355260611,-0.13201580941677094,0.035715050995349884,-0.1213662400841713,0.032630570232868195,0.04873481020331383,-0.17909474670886993,0.17584791779518127,-0.1285741776227951,0.037273526191711426,-0.14143159985542297,0.1436394453048706,0.09279419481754303,0.1490941047668457,0.07467692345380783,-0.09409292787313461,0.09675531834363937,0.13350935280323029,-0.19415999948978424,-0.18454940617084503,0.15182143449783325,-0.043604832142591476,0.01301164273172617,0.20143288373947144,0.015333120711147785],"The discount offered was enticing.":[-0.12655314803123474,-0.0466570146381855,-0.27802109718322754,0.03967156261205673,0.13155940175056458,0.05116845667362213,-0.15833696722984314,0.4144703149795532,-0.007458427920937538,-0.06921420991420746,0.13062800467014313,-0.044503167271614075,-0.13924339413642883,-0.1716093271970749,0.2568318843841553,0.13225793838500977,0.009481299668550491,-0.024609174579381943,-0.1264367252588272,0.16066545248031616,0.01923910528421402,0.10082339495420456,-0.02124742418527603,-0.02405615895986557,-0.15007084608078003,-0.19244927167892456,-0.2273765504360199,-0.2924576997756958,0.13807915151119232,-0.05678592622280121,0.03731397166848183,-0.12795023620128632,-0.050906501710414886,-0.10140551626682281,-0.08929739147424698,0.2691728472709656,-0.06770069897174835,0.07241588085889816,0.13260720670223236,-0.12201260775327682,0.01567361317574978,-0.158919095993042,0.1357506662607193,0.07381296902894974,0.01432018168270588,0.15472781658172607,0.0062141441740095615,-0.08859884738922119,0.01254471205174923,0.14797520637512207],"The payment process was secure and efficient.":[-0.04701301082968712,-0.20167900621891022,-0.22099372744560242,-0.05536692962050438,0.03149012476205826,0.049234796315431595,-0.02104772813618183,0.1948062777519226,0.004417652729898691,-0.11180031299591064,0.25831976532936096,-0.1503705382347107,-0.14669717848300934,-0.15866521000862122,0.07601473480463028,-0.03744451329112053,-0.1256050169467926,-0.004232503939419985,-0.19717617332935333,-0.07204513996839523,0.07216363400220871,0.23426520824432373,0.005728506948798895,-0.08347994089126587,-0.09248558431863785,-0.16150909662246704,-0.10895642638206482,-0.3507460951805115,-0.1641159951686859,-0.1695667803287506,0.21696490049362183,-0.1385210007429123,0.196346715092659,-0.025669043883681297,-0.07808840274810791,-0.0023291732650250196,-0.03386003151535988,0.14717115461826324,0.06078808754682541,-0.0358448289334774,-0.1290413737297058,0.17335861921310425,-0.08033981174230576,0.1285673975944519,-0.040229152888059616,0.11511818319559097,0.10747523605823517,-0.3336827754974365,0.09313730895519257,-0.002255113562569022]}    Your task is to write a Python function most_similar(embeddings) that will calculate the cosine similarity between each pair of these embeddings and return the pair that has the highest similarity. The result should be a tuple of the two phrases that are most similar.

    Write your Python code here:
    import numpy


    def most_similar(embeddings):
        # Your code here
        return (phrase1, phrase2)
         